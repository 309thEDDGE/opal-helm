# Optionally override the fully qualified name
fullnameOverride: ""

# Optionally override the name
nameOverride: ""

# Namespace to install traefik
namespace: "opal"

# domain info for ingress
baseDns: ""
domainExtension: ""

# The number of replicas to create (has no effect if autoscaling enabled)
replicas: 1

image:
  registry: registry1.dso.mil/ironbank/opensource/traefik
  # The Traefik image repository
  repository: traefik
  # Overrides the Traefik image tag whose default is the chart appVersion
  tag: "v3.0.4"
  # The Traefik image pull policy
  pullPolicy: IfNotPresent

# Image pull secrets for the Pod
imagePullSecrets:
- name: regcred

tls:
  nameOverride: ""

# Indicates whether information about services should be injected into Pod's environment variables, matching the syntax of Docker links
enableServiceLinks: true

# Pod restart policy. One of `Always`, `OnFailure`, or `Never`
restartPolicy: Always


serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true

rbac:
  enabled: true


# SecurityContext for the entire Pod. Every container running in the Pod will inherit this SecurityContext. This might be relevant when other components of the environment inject additional containers into running Pods (service meshes are the most prominent example for this)
podSecurityContext:
  fsGroup: 1000

# SecurityContext for the Traefik container
securityContext:
  runAsUser: 1000
  runAsNonRoot: true
  capabilities:
    drop:
      - ALL


terminationGracePeriodSeconds: 60

# The internal Kubernetes cluster domain
clusterDomain: cluster.local



## Pod resource requests and limits
#resources:
#  requests:
#    cpu: "1"
#    memory: "1Gi"
#  limits:
#    cpu: "1"
#    memory: "1Gi"

#### starupScripts removed. Not relevant for Quarkus distribution ####

# Add additional volumes, e.g. for custom themes
extraVolumes: ""


# Add additional volumes mounts, e.g. for custom themes
extraVolumeMounts: ""


# Pod disruption budget
podDisruptionBudget: {}
#  maxUnavailable: 1
#  minAvailable: 1

# Configuration for secrets that should be created
## The secrets can also be independently created separate from this helm chart.
## for example with a gitops tool like flux with a kustomize overlay.

service:
  # Annotations for headless and HTTP Services
  annotations: {}
  # Additional labels for headless and HTTP Services
  labels: {}
  # key: value
  # Optional IP for the load balancer. Used for services of type LoadBalancer only
  loadBalancerIP: ""
  # The http Service port
  httpPort: 80

  extraPorts: []
  # When using Service type LoadBalancer, you can restrict source ranges allowed
  # to connect to the LoadBalancer, e.g. will result in Security Groups
  # (or equivalent) with inbound source ranges allowed to connect
  loadBalancerSourceRanges: []
  # When using Service type LoadBalancer, you can preserve the source IP seen in the container
  # by changing the default (Cluster) to be Local.
  # See https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  externalTrafficPolicy: "Cluster"
  # Session affinity
  # See https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-userspace
  sessionAffinity: ""
  # Session affinity config
  sessionAffinityConfig: {}

metrics:
  enabled: false
  port: 8082

ingress:
  # If `true`, an Ingress is created
  enabled: false
  # The Service port targeted by the Ingress
  servicePort: http
  # Ingress annotations
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
    ## Resolve HTTP 502 error using ingress-nginx:
    ## See https://www.ibm.com/support/pages/502-error-ingress-traefik-response
    # nginx.ingress.kubernetes.io/proxy-buffer-size: 128k

  # Additional Ingress labels
  labels: {}
   # List of rules for the Ingress
  rules:
      # Ingress host
      host: ""
      # Paths for the host
      paths:
        - path: /
          pathType: Prefix
# Example TLS configuration
#   tls:
#     - hosts:
#         - traefik.example.com
#       secretName: ""

## Network policy configuration
networkPolicy:
  # If true, the Network policies are deployed
  enabled: false

  # Additional Network policy labels
  labels: {}

  # Define all other external allowed source
  # See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#networkpolicypeer-v1-networking-k8s-io
  extraFrom: []

serviceMonitor:
  # If `true`, a ServiceMonitor resource for the prometheus-operator is created
  enabled: true
  # Optionally sets a target namespace in which to deploy the ServiceMonitor resource
  namespace: "monitoring"
  # Optionally sets a namespace for the ServiceMonitor
  namespaceSelector: |
    matchNames:
      - {{ .Values.namespace }}
  # Annotations for the ServiceMonitor
  annotations: {}
  # Additional labels for the ServiceMonitor
  labels: {}
  # Interval at which Prometheus scrapes metrics
  interval: 10s
  # Timeout for scraping
  scrapeTimeout: 10s
  # The path at which metrics are served
  path: /metrics
  # The Service port at which metrics are served
  port: metrics
  # added by Big Bang to support Istio mTLS
  scheme: ""
  tlsConfig: {}

autoscaling:
  # If `true`, an autoscaling/v2beta2 HorizontalPodAutoscaler resource is created (requires Kubernetes 1.18 or above)
  # Autoscaling seems to be most reliable when using KUBE_PING service discovery (see README for details)
  # This disables the `replicas` field in the StatefulSet
  enabled: false
  # Additional HorizontalPodAutoscaler labels
  labels: {}
  # The minimum and maximum number of replicas for the Traefik StatefulSet
  minReplicas: 3
  maxReplicas: 10
  # The metrics to use for scaling
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
  # The scaling policy to use. This will scale up quickly but only scale down a single Pod per 5 minutes.
  # This is important because caches are usually only replicated to 2 Pods and if one of those Pods is terminated this will give the cluster time to recover.
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300

